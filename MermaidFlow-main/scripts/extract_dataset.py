from typing import Set, List, Literal, Dict
from pydantic import BaseModel, Field
from typing import Optional, List
import dspy
from dspy.teleprompt import MIPROv2

from benchmarks.benchmark import BaseBenchmark
from benchmarks.drop import DROPBenchmark
from benchmarks.gsm8k import GSM8KBenchmark
from benchmarks.hotpotqa import HotpotQABenchmark
from benchmarks.humaneval import HumanEvalBenchmark
from benchmarks.math import MATHBenchmark
from benchmarks.mbpp import MBPPBenchmark
from benchmarks.aime import AIMEBenchmark


class HardProblem(BaseModel):
    """
    Model representing a difficult problem that the system failed to solve correctly.
    Used to collect challenging examples for optimization and improvement.
    """
    question: str | None = Field(None, description="The original problem or question text that was presented to the system")
    resoning: str | None = Field(None, description="The reasoning or solution process generated by the system")
    answer: str | None = Field(None, description="The expected correct answer or solution to the problem")


def transfer_format(calls_item):
    """Convert a call item to HardProblem format."""
    convert = dict(calls_item.output)
    return HardProblem(question=convert["input_text"], resoning=convert["expected_output"], answer=convert["expected_output"])


def collect_hard_problems(calls, processed_records: Set[str]) -> tuple[List[HardProblem], Set[str]]:
    """
    Collect problems that were not solved correctly.
    
    Args:
        calls: List of call records to process
        processed_records: Set of already processed problem texts
    
    Returns:
        Tuple of (list of hard problems, updated processed records)
    """
    # TODO: if the whole validation process doesn't solve this problem, than add it to the hard set.
    hard_problems = []
    for call in calls:
        # Skip if we've already seen this problem
        if call.inputs["problem"]["problem"] in processed_records:
            continue
        
        # Only collect problems that were not solved correctly
        try:
            if not call.output["uni_score"]:
                processed_records.add(call.output["input_text"])
                hard_problems.append(transfer_format(call))
        except TypeError:
            print(f"TypeError encountered when processing call output for problem: {call.inputs.get('problem', {}).get('problem', 'Unknown problem')}. Missing or invalid data structure.")
            continue
    return hard_problems, processed_records

class PromptOptimizationConfig(BaseModel):
    """Configuration parameters for prompt optimization process."""
    dataset_type: str = Field("")
    trainset: Optional[List] = Field(None, description="Validation dataset for optimization")
    valset: Optional[List] = Field(None, description="Validation dataset for optimization")
    num_trials: int = Field(3, description="Number of optimization trials to run")
    max_bootstrapped_demos: Optional[int] = Field(None, description="Maximum number of bootstrapped demonstrations")
    max_labeled_demos: Optional[int] = Field(None, description="Maximum number of labeled demonstrations")
    seed: Optional[int] = Field(None, description="Random seed for reproducibility")
    minibatch: bool = Field(True, description="Whether to use minibatch evaluation")
    minibatch_size: int = Field(25, description="Size of minibatches for evaluation")
    minibatch_full_eval_steps: int = Field(10, description="Frequency of full evaluation during minibatch training")
    program_aware_proposer: bool = Field(True, description="Whether to use program-aware instruction proposer")
    data_aware_proposer: bool = Field(True, description="Whether to use data-aware instruction proposer")
    view_data_batch_size: int = Field(10, description="Batch size for viewing data during optimization")
    tip_aware_proposer: bool = Field(True, description="Whether to use tip-aware instruction proposer")
    fewshot_aware_proposer: bool = Field(True, description="Whether to use few-shot aware instruction proposer")
    requires_permission_to_run: bool = Field(True, description="Whether to require user confirmation before running")
    api_key: str = Field("")

DatasetType = Literal["HumanEval", "MBPP", "GSM8K", "MATH", "HotpotQA", "DROP", "aime"]

dataset_configs: Dict[DatasetType, BaseBenchmark] = {
            "GSM8K": GSM8KBenchmark,
            "MATH": MATHBenchmark,
            "HumanEval": HumanEvalBenchmark,
            "HotpotQA": HotpotQABenchmark,
            "MBPP": MBPPBenchmark,
            "DROP": DROPBenchmark,
            "aime": AIMEBenchmark,
        }

def get_dataset(train_set: list):
    # if len(train_set) < 10:
    #     pass

    dataset = [dspy.Example(**dict(x)).with_inputs("question") for x in train_set]
    # TODO: classify the dataset, and pick one of them to create new training dataset

    return dataset



def get_single_new_instruction(prompt_setting: PromptOptimizationConfig):
    lm = dspy.LM('openai/gpt-4o-mini', api_key=prompt_setting.api_key)
    dspy.configure(lm=lm)
    metric_func = dataset_configs[prompt_setting.dataset_type].calculate_score

    def adapter(func):
        
        def new_func(gold, pred, trace=None):
            is_match, _ = func(expected_output=gold["answer"], prediction=pred["answer"])
            return is_match == 1

        return new_func
    
    metric_func = adapter(metric_func)

    train_set = get_dataset(prompt_setting.trainset)

    # teleprompter = MIPROv2(metric=metric_func, auto="medium")
    teleprompter = MIPROv2(metric=metric_func, auto="light")
    optimized_program = teleprompter.compile(
        dspy.ChainOfThought("question -> answer"),
        trainset=train_set,
        valset=prompt_setting.valset,
        num_trials=prompt_setting.num_trials,
        requires_permission_to_run=False
    )
    return optimized_program


    
    
# import dspy
# from dspy.datasets.gsm8k import GSM8K, gsm8k_metric

# # Import the optimizer
# from dspy.teleprompt import MIPROv2

# # Initialize the LM
# lm = dspy.LM('openai/gpt-4o-mini', api_key='YOUR_OPENAI_API_KEY')
# dspy.configure(lm=lm)

# # Initialize optimizer
# teleprompter = MIPROv2(
#     metric=gsm8k_metric,
#     auto="medium", # Can choose between light, medium, and heavy optimization runs
# )

# # Optimize program
# print(f"Optimizing program with MIPROv2...")

# optimized_program = teleprompter.compile(
#     dspy.ChainOfThought("question -> answer"),
#     trainset=gsm8k.train,
#     requires_permission_to_run=False,
# )

# # Save optimize program for future use
# optimized_program.save(f"optimized.json")